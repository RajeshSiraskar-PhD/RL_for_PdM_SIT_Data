{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f03fe92",
   "metadata": {},
   "source": [
    "## RL for Predictive Maintenance - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bd9c8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f45d48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, Dict, Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e49a4f",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a12b1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Parameters ---\n",
    "# Data and Environment settings\n",
    "SENSOR_DATA = 'PROC_10'\n",
    "DATA_FILE = f'data\\{SENSOR_DATA}.csv'\n",
    "TEST_FILE = f'data\\TEST_64.csv'\n",
    "WEAR_THRESHOLD = 300.0  # Threshold for tool wear before it's considered failed\n",
    "\n",
    "# Reward structure\n",
    "R1_CONTINUE = 2.0      # Reward for continuing with a healthy tool\n",
    "R2_REPLACE = -5.0      # Reward for a timely replacement\n",
    "R3_VIOLATION = -30.0   # Penalty for violating the wear threshold\n",
    "\n",
    "# Training settings\n",
    "EPISODES = 500          # Total number of episodes to train each agent\n",
    "LEARNING_RATE = 5e-3    # Learning rate for optimizers\n",
    "GAMMA = 0.99            # Discount factor for future rewards\n",
    "\n",
    "# Plotting settings\n",
    "SMOOTH_WINDOW = 50      # Window size for moving average smoothing on plots\n",
    "\n",
    "# Model saving\n",
    "SAVE_MODEL = True\n",
    "REINFORCE_MODEL = f'models/{SENSOR_DATA}_REINFORCE_Model.h5'\n",
    "REINFORCE_AM_MODEL = f'models/{SENSOR_DATA}_REINFORCE_AM_Model.h5'\n",
    "PPO_MODEL = f'models/{SENSOR_DATA}_PPO_Model.zip'  # PPO uses .zip extension by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f78bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "67ef837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 2: REINFORCE Algorithm Implementation ---\n",
    "# A custom implementation of the REINFORCE algorithm with a structure\n",
    "# similar to Stable Baselines3 for easy comparison.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x)\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    A custom REINFORCE agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, policy, env, learning_rate=0.001, gamma=0.99, model_file=None):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.model_name = \"REINFORCE\"\n",
    "        self.model_file = model_file\n",
    "\n",
    "    def predict(self, obs: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current observation.\n",
    "        Uses the same action selection logic as training for consistency.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Important to disable gradients for prediction\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            probs = self.policy(state_tensor)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()  # Sample action like in training\n",
    "            return action.item()        \n",
    "\n",
    "    def learn(self, total_episodes: int) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        Trains the agent for a given number of episodes.\n",
    "        \"\"\"\n",
    "        print(f\"--- Training {self.model_name} ---\")\n",
    "        # Metrics collectors\n",
    "        all_rewards = []\n",
    "        all_violations = []\n",
    "        all_replacements = []\n",
    "        all_margins = []\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            episode_info = {'violation': 0, 'replacement': 0, 'margin': np.nan}\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                probs = self.policy(state_tensor)\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample()\n",
    "\n",
    "                log_prob = dist.log_prob(action)\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "                obs, reward, terminated, truncated, info = self.env.step(action.item())\n",
    "                rewards.append(reward)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if info.get('violation'): episode_info['violation'] = 1\n",
    "                if info.get('replacement'): episode_info['replacement'] = 1\n",
    "                if not np.isnan(info.get('margin')): episode_info['margin'] = info.get('margin')\n",
    "            \n",
    "            # Collect metrics for this episode\n",
    "            all_rewards.append(sum(rewards))\n",
    "            all_violations.append(episode_info['violation'])\n",
    "            all_replacements.append(episode_info['replacement'])\n",
    "            all_margins.append(episode_info['margin'])\n",
    "\n",
    "            # Calculate discounted returns\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32)\n",
    "            \n",
    "            # Normalize returns for stability, but only if there's more than one step\n",
    "            # to avoid std() of a single item becoming NaN.\n",
    "            if len(returns) > 1:\n",
    "                returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "            # Calculate policy loss and update weights\n",
    "            policy_loss = []\n",
    "            for log_prob, G in zip(log_probs, returns):\n",
    "                policy_loss.append(-log_prob * G)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss = torch.cat(policy_loss).sum()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                print(f\"Episode {episode + 1}/{total_episodes}, Total Reward: {sum(rewards):.2f}\")\n",
    "        \n",
    "        print(\"--- Training Complete ---\")\n",
    "        \n",
    "        # Save model if file path was provided\n",
    "        if self.model_file is not None:\n",
    "            try:\n",
    "                # Create directory if it doesn't exist\n",
    "                os.makedirs(os.path.dirname(self.model_file), exist_ok=True)\n",
    "                # Save the model\n",
    "                torch.save({\n",
    "                    'policy_state_dict': self.policy.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'input_dim': self.policy.net[0].in_features,\n",
    "                    'output_dim': self.policy.net[-2].out_features\n",
    "                }, self.model_file)\n",
    "                print(f\"Model saved successfully to: {self.model_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            \"rewards\": all_rewards,\n",
    "            \"violations\": all_violations,\n",
    "            \"replacements\": all_replacements,\n",
    "            \"margins\": all_margins\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba8015",
   "metadata": {},
   "source": [
    "### Custom RL Environment for milling tool maintenance\n",
    "- OpenAI's Gymnasium open standards format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d534a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Custom Gymnasium Environment ---\n",
    "# We define the environment for the milling tool maintenance problem.\n",
    "\n",
    "class MT_Env(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium environment for Milling Tool Predictive Maintenance.\n",
    "\n",
    "    The agent observes the current tool wear and decides whether to continue\n",
    "    using the tool or replace it.\n",
    "\n",
    "    Observation Space: A single continuous value representing the current tool wear.\n",
    "    Action Space: Two discrete actions - 0 (CONTINUE) or 1 (REPLACE_TOOL).\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, data_file: str, wear_threshold: float, r1: float, r2: float, r3: float):\n",
    "        \"\"\"\n",
    "        Initializes the environment.\n",
    "\n",
    "        Args:\n",
    "            data_file (str): Path to the CSV file with sensor data and tool wear.\n",
    "            wear_threshold (float): The maximum allowable tool wear.\n",
    "            r1 (float): Reward for continuing.\n",
    "            r2 (float): Reward for a timely replacement.\n",
    "            r3 (float): Penalty for a threshold violation.\n",
    "        \"\"\"\n",
    "        super(MT_Env, self).__init__()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        if not os.path.exists(data_file):\n",
    "            raise FileNotFoundError(f\"Data file not found at {data_file}. Please ensure the file is in the correct directory.\")\n",
    "        self.df = pd.read_csv(data_file)\n",
    "        self.max_steps = len(self.df) - 1\n",
    "\n",
    "        # Environment parameters\n",
    "        self.wear_threshold = wear_threshold\n",
    "        self.r1_continue = r1\n",
    "        self.r2_replace = r2\n",
    "        self.r3_violation = r3\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(2)  # 0: CONTINUE, 1: REPLACE_TOOL\n",
    "        # The observation is the current tool wear, normalized for better learning\n",
    "        self.observation_space = spaces.Box(low=0, high=self.wear_threshold * 1.5, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Internal state\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state for a new episode.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        initial_observation = self._get_observation()\n",
    "        return initial_observation, {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment.\n",
    "        \"\"\"\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "        info = {'violation': False, 'replacement': False, 'margin': np.nan}\n",
    "\n",
    "        current_wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "\n",
    "        if action == 1:  # Action: REPLACE_TOOL\n",
    "            reward = self.r2_replace\n",
    "            terminated = True\n",
    "            info['replacement'] = True\n",
    "            info['margin'] = self.wear_threshold - current_wear\n",
    "        elif action == 0:  # Action: CONTINUE\n",
    "            self.current_step += 1\n",
    "            if self.current_step > self.max_steps:\n",
    "                # Reached the end of data without replacement or violation\n",
    "                truncated = True\n",
    "                reward = 0 # No reward or penalty if data runs out\n",
    "            else:\n",
    "                next_wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "                if next_wear >= self.wear_threshold:\n",
    "                    reward = self.r3_violation\n",
    "                    terminated = True\n",
    "                    info['violation'] = True\n",
    "                    # $$$ Avoid NANs for margin. Even if NOT replaced, see what the margin is\n",
    "                    info['margin'] = self.wear_threshold - current_wear\n",
    "                else:\n",
    "                    reward = self.r1_continue\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gets the current observation from the dataframe.\n",
    "        \"\"\"\n",
    "        wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "        return np.array([wear], dtype=np.float32)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Renders the environment state (optional, for visualization).\n",
    "        \"\"\"\n",
    "        wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "        print(f\"Step: {self.current_step}, Tool Wear: {wear:.2f}\")\n",
    "\n",
    "\n",
    "class AM_Env(MT_Env):\n",
    "    \"\"\"\n",
    "        Attention-augmented variant of MT_Env.\n",
    "\n",
    "        - Uses the same sensor features as the dataset.\n",
    "        Computes attention weights via KernelRidge(kernel='linear') fitted to features -> tool_wear,\n",
    "        The primal coefficients are used to derive per-feature importances (absolute value, normalized).\n",
    "        - Observation returned is the normalized feature vector multiplied elementwise by attention weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init_(self, data_file: str, wear_threshold: float, r1: float, r2: float, r3: float, kr_alpha: float = 1.0):\n",
    "        super(AM_Env, self).__init__(data_file, wear_threshold, r1, r2, r3)\n",
    "\n",
    "        # feature list (same as suggested earlier)\n",
    "        self.features = ['vib_Spindle', 'Vib_Table', 'Sound_Spindle', 'Sound_table', 'X_Load_Cell', 'Y_Load_Cell', 'Z_Load_Cell', 'Current']\n",
    "\n",
    "        # ensure features exist\n",
    "        missing = [f for f in self.features if f not in self.df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing features in dataset: {missing}\")\n",
    "\n",
    "        # compute per-feature normalization stats\n",
    "        vals_df = self.df[self.features].astype(np.float32)\n",
    "        self.feature_means = vals_df.mean(axis=0).to_numpy(dtype=np.float32)\n",
    "        self.feature_stds = vals_df.std(axis=0).replace(0, 1.0).to_numpy(dtype=np.float32)\n",
    "\n",
    "        # prepare training data for KernelRidge\n",
    "        X = (vals_df - self.feature_means) / (self.feature_stds + 1e-9)\n",
    "        x = x.to_numpy(dtype=np.float32)\n",
    "        y = self.df['tool_wear'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # fit KernelRidge (linear) to estimate feature importances\n",
    "        try:\n",
    "            from sklearn.kernel_ridge import KernelRidge\n",
    "        except Exception as e:\n",
    "            raise ImportError(\"scikit-learn is required for AM_Env. Install with `pip install scikit-learn\") from e\n",
    "\n",
    "        self._kr_model = KernelRidge(kernel='linear', alpha=kr_alpha)\n",
    "        self._kr_model.fit(X, y)\n",
    "\n",
    "        # compute primal coefficients from dual_coef_ and x_fit_\n",
    "        # coef = x_fit _. T @ dual_coef_ (shape: n_features,)\n",
    "        dual=np.asarray(self._kr_model.dual_coef_).reshape(-1, 1) # (n_train, 1)\n",
    "        x_fit = np.asarray(self._kr_model.x_fit_) # (n_train, n_features)\n",
    "        coef = (x_fit.T @ dual).ravel() # (n_features,)\n",
    "\n",
    "        attn = np.abs(coef)\n",
    "        if attn.sum() == 0:\n",
    "            attn = np.ones_like(attn)\n",
    "        attn = attn / (attn.sum() + 1e-12)\n",
    "        self.attention_weights = attn.astype(np.float32)\n",
    "\n",
    "        # observation space: attention-weighted feature vector\n",
    "        self.observation_space = spaces.Box(low =- np.inf, high=np.inf, shape=(len(self.features),), dtype=np.float32)\n",
    "\n",
    "    def get_observation(self) -> np.ndarray:\n",
    "        \"\"\"Return the attention-weighted, normalized feature vector for current step.\"\"\"\n",
    "        vals = self.df.loc[self.current_step, self.features].to_numpy(dtype=np.float32)\n",
    "        norm = (vals - self.feature_means) / (self.feature_stds + 1e-9)\n",
    "        weighted = norm * self.attention_weights\n",
    "        return weighted.astype(np.float32)\n",
    "    \n",
    "    def recompute_attention(self, window: int = None, kr_alpha: float = None):\n",
    "        \"\"\"\n",
    "            Optionally recompute attention weights:\n",
    "            - If window'is None -> use full dataset (same as init).\n",
    "            - If window is int -> use last 'window rows up to current_step to compute local attention.\n",
    "            - kr_alpha overrides regularization when refitting.\n",
    "        \"\"\"\n",
    "        \n",
    "        if window is None:\n",
    "            df_slice = self.df[self.features]\n",
    "        else:\n",
    "            start = max(0, self.current_step - window + 1)\n",
    "            df_slice = self.df.loc[start:self.current_step, self.features]\n",
    "            if len(df_slice) < 2:\n",
    "                df_slice = self.df[self.features] # fallback to full data if slice too small\n",
    "\n",
    "        x = ((df_slice - self.feature_means) / (self.feature_stds + 1e-9)).to_numpy(dtype=np.float32)\n",
    "        y = self.df.loc[df_slice.index, 'tool_wear'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        from sklearn.kernel_ridge import KernelRidge\n",
    "        model = KernelRidge(kernel='linear', alpha=(kr_alpha if kr_alpha is not None else self._kr_model.alpha))\n",
    "        model.fit(x, y)\n",
    "        dual = np.asarray(model.dual_coef_).reshape(-1, 1)\n",
    "        x_fit = np.asarray(model.x_fit )\n",
    "        coef = (x_fit.T @ dual).ravel()\n",
    "        attn = np.abs(coef)\n",
    "        if attn.sum() == 0:\n",
    "            attn = np.ones_like(attn)\n",
    "        attn = attn / (attn.sum() + 1e-12)\n",
    "        self.attention_weights = attn.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "325c1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_saved_model(\n",
    "    model_file: str, \n",
    "    test_data_file: str, \n",
    "    wear_threshold: float, \n",
    "    r1: float, \n",
    "    r2: float, \n",
    "    r3: float,\n",
    "    model_type: str = \"REINFORCE\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Tests a saved model (REINFORCE or PPO) on new sensor data and visualizes the results.\n",
    "    \n",
    "    Args:\n",
    "        model_file (str): Path to the saved model file\n",
    "        test_data_file (str): Path to the new sensor data file\n",
    "        wear_threshold (float): Threshold for tool wear\n",
    "        r1, r2, r3 (float): Reward parameters for the environment\n",
    "        model_type (str): Type of model to test (\"REINFORCE\" or \"PPO\")\n",
    "    \"\"\"\n",
    "    # Create test environment\n",
    "    test_env = MT_Env(\n",
    "        data_file=test_data_file,\n",
    "        wear_threshold=wear_threshold,\n",
    "        r1=r1, r2=r2, r3=r3\n",
    "    )\n",
    "    \n",
    "    # Load the appropriate model based on type\n",
    "    if model_type.upper() == \"REINFORCE\":\n",
    "        policy = PolicyNetwork(\n",
    "            input_dim=test_env.observation_space.shape[0],\n",
    "            output_dim=test_env.action_space.n\n",
    "        )\n",
    "        agent = REINFORCE(policy, test_env)\n",
    "        try:\n",
    "            checkpoint = torch.load(model_file)\n",
    "            agent.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "            print(f\"REINFORCE model loaded successfully from: {model_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading REINFORCE model: {str(e)}\")\n",
    "            return\n",
    "    elif model_type.upper() == \"PPO\":\n",
    "        try:\n",
    "            agent = PPO.load(model_file)\n",
    "            print(f\"PPO model loaded successfully from: {model_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading PPO model: {str(e)}\")\n",
    "            return\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be either 'REINFORCE' or 'PPO'\")\n",
    "    \n",
    "    # Test metrics\n",
    "    total_steps = 0\n",
    "    total_replacements = 0\n",
    "    total_violations = 0\n",
    "    margins = []\n",
    "    replacement_points = []\n",
    "    violation_points = []\n",
    "    wear_trajectory = []\n",
    "    \n",
    "    # Run test episode\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get action based on model type\n",
    "        if model_type.upper() == \"REINFORCE\":\n",
    "            action = agent.predict(obs)\n",
    "        else:  # PPO\n",
    "            action, _states = agent.predict(obs, deterministic=True)\n",
    "            \n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        wear = obs[0]\n",
    "        wear_trajectory.append(wear)\n",
    "        \n",
    "        if info.get('replacement'):\n",
    "            total_replacements += 1\n",
    "            replacement_points.append((step, wear))\n",
    "            margins.append(wear_threshold - wear)\n",
    "        \n",
    "        if info.get('violation'):\n",
    "            total_violations += 1\n",
    "            violation_points.append((step, wear))\n",
    "            margins.append(wear_threshold - wear)\n",
    "        \n",
    "        step += 1\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    total_steps = step\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'total_steps': total_steps,\n",
    "        'replacement_rate': total_replacements / total_steps,\n",
    "        'violation_rate': total_violations / total_steps,\n",
    "        'avg_margin': np.mean(margins) if margins else np.nan,\n",
    "        'min_margin': np.min(margins) if margins else np.nan\n",
    "    }\n",
    "    \n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    fig.suptitle(f'{model_type} Model Test Results', fontsize=14)\n",
    "    \n",
    "    # Plot 1: Wear trajectory with events\n",
    "    ax1.plot(wear_trajectory, 'b-', alpha=0.6, label='Tool Wear')\n",
    "    ax1.axhline(y=wear_threshold, color='r', linestyle='--', alpha=0.6, label='Wear Threshold')\n",
    "    \n",
    "    # Plot replacement and violation points\n",
    "    if replacement_points:\n",
    "        points = np.array(replacement_points)\n",
    "        ax1.scatter(points[:, 0], points[:, 1], color='g', marker='^', \n",
    "                   label='Replacements', alpha=0.6)\n",
    "    if violation_points:\n",
    "        points = np.array(violation_points)\n",
    "        ax1.scatter(points[:, 0], points[:, 1], color='r', marker='x', \n",
    "                   label='Violations', alpha=0.6)\n",
    "    \n",
    "    ax1.set_title('Tool Wear Trajectory')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Wear')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Margins histogram\n",
    "    if margins:\n",
    "        ax2.hist(margins, bins=20, alpha=0.6, color='purple')\n",
    "        ax2.axvline(x=0, color='r', linestyle='--', alpha=0.6)\n",
    "        ax2.set_title('Distribution of Margins at Decision Points')\n",
    "        ax2.set_xlabel('Margin (Threshold - Wear)')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{model_type} Test Results:\")\n",
    "    print(f\"Total Steps: {metrics['total_steps']}\")\n",
    "    print(f\"Replacement Rate: {metrics['replacement_rate']:.4f}\")\n",
    "    print(f\"Violation Rate: {metrics['violation_rate']:.4f}\")\n",
    "    print(f\"Average Margin: {metrics['avg_margin']:.2f}\")\n",
    "    print(f\"Minimum Margin: {metrics['min_margin']:.2f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a39de",
   "metadata": {},
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5cf25b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created and checked successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "try:\n",
    "    # Create a base environment for the custom REINFORCE agent\n",
    "    reinforce_env = MT_Env(\n",
    "        data_file=DATA_FILE,\n",
    "        wear_threshold=WEAR_THRESHOLD,\n",
    "        r1=R1_CONTINUE,\n",
    "        r2=R2_REPLACE,\n",
    "        r3=R3_VIOLATION\n",
    "    )\n",
    "    # It's good practice to check if the custom environment is valid\n",
    "    check_env(reinforce_env)\n",
    "    print(\"Environment created and checked successfully.\")\n",
    "\n",
    "    # Create a separate, monitored environment for the Stable Baselines3 PPO agent\n",
    "    ppo_env = Monitor(MT_Env(\n",
    "        data_file=DATA_FILE,\n",
    "        wear_threshold=WEAR_THRESHOLD,\n",
    "        r1=R1_CONTINUE,\n",
    "        r2=R2_REPLACE,\n",
    "        r3=R3_VIOLATION\n",
    "    ))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    # Exit if the data file is not found\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a847b",
   "metadata": {},
   "source": [
    "### Environment with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b86da7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base environment for the custom REINFORCE agent\n",
    "reinforce_am_env = AM_Env(\n",
    "    data_file=DATA_FILE,\n",
    "    wear_threshold=WEAR_THRESHOLD,\n",
    "    r1=R1_CONTINUE,\n",
    "    r2=R2_REPLACE,\n",
    "    r3=R3_VIOLATION)\n",
    "\n",
    "check_env(reinforce_am_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8aaecf",
   "metadata": {},
   "source": [
    "### Test model saving and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc96f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def test_all_models(\n",
    "    test_data_file: str,\n",
    "    wear_threshold: float = WEAR_THRESHOLD,\n",
    "    r1: float = R1_CONTINUE,\n",
    "    r2: float = R2_REPLACE,\n",
    "    r3: float = R3_VIOLATION,\n",
    "    ppo_model_file: str = PPO_MODEL,\n",
    "    reinforce_model_file: str = REINFORCE_MODEL,\n",
    "    reinforce_am_model_file: str = REINFORCE_AM_MODEL,\n",
    ") -> \"pd.DataFrame\":\n",
    "    \"\"\"\n",
    "    Test three models (PPO, REINFORCE, REINFORCE+Attention) on a test CSV.\n",
    "    - Treats the 'optimal' / ground-truth action at time t as: replace (1)\n",
    "      iff the next timestep's wear >= wear_threshold (prevents violation).\n",
    "      (Last timestep -> 0)\n",
    "    - Returns a pandas DataFrame with precision, recall, f1, accuracy and\n",
    "      average replacement margin for each model.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    # Helper to compute per-episode metrics for a model interacting with an env\n",
    "    def run_episode(env, action_getter):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        replacement_margins = []\n",
    "\n",
    "        while not done:\n",
    "            # Determine ground-truth: replace if next wear >= threshold\n",
    "            cur = env.current_step\n",
    "            next_idx = cur + 1\n",
    "            if next_idx <= env.max_steps:\n",
    "                next_wear = env.df.loc[next_idx, \"tool_wear\"]\n",
    "                true_action = 1 if next_wear >= wear_threshold else 0\n",
    "            else:\n",
    "                true_action = 0\n",
    "\n",
    "            # get model action (0/1)\n",
    "            action = int(action_getter(obs))\n",
    "\n",
    "            # step environment with model action\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # record\n",
    "            y_true.append(true_action)\n",
    "            y_pred.append(action)\n",
    "\n",
    "            # if model replaced, record margin (info['margin'] provided by env)\n",
    "            if info.get(\"replacement\"):\n",
    "                if not np.isnan(info.get(\"margin\", np.nan)):\n",
    "                    replacement_margins.append(float(info.get(\"margin\")))\n",
    "\n",
    "        # classification metrics (handle zero-division)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        avg_margin = float(np.mean(replacement_margins)) if replacement_margins else float(\"nan\")\n",
    "\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": acc, \"avg_margin\": avg_margin}\n",
    "\n",
    "    # Load test environments\n",
    "    mt_env = MT_Env(data_file=test_data_file, wear_threshold=wear_threshold, r1=r1, r2=r2, r3=r3)\n",
    "    am_env = AM_Env(data_file=test_data_file, wear_threshold=wear_threshold, r1=r1, r2=r2, r3=r3)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1) PPO\n",
    "    try:\n",
    "        ppo_agent = PPO.load(ppo_model_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PPO model '{ppo_model_file}': {e}\")\n",
    "        ppo_agent = None\n",
    "\n",
    "    if ppo_agent is not None:\n",
    "        def ppo_action_getter(obs):\n",
    "            # stable-baselines expects 1D array; returns action (int)\n",
    "            a, _ = ppo_agent.predict(obs, deterministic=True)\n",
    "            return int(a)\n",
    "        results[\"PPO\"] = run_episode(mt_env, ppo_action_getter)\n",
    "    else:\n",
    "        results[\"PPO\"] = {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan, \"accuracy\": np.nan, \"avg_margin\": np.nan}\n",
    "\n",
    "    # 2) REINFORCE (raw sensor)\n",
    "    try:\n",
    "        # instantiate policy matching env obs dim\n",
    "        policy_rf = PolicyNetwork(mt_env.observation_space.shape[0], mt_env.action_space.n)\n",
    "        ckpt = torch.load(reinforce_model_file)\n",
    "        policy_rf.load_state_dict(ckpt[\"policy_state_dict\"])\n",
    "        policy_rf.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading REINFORCE model '{reinforce_model_file}': {e}\")\n",
    "        policy_rf = None\n",
    "\n",
    "    if policy_rf is not None:\n",
    "        def rf_action_getter(obs):\n",
    "            with torch.no_grad():\n",
    "                t = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                probs = policy_rf(t).squeeze(0).cpu().numpy()\n",
    "                # deterministic -> argmax\n",
    "                return int(np.argmax(probs))\n",
    "        results[\"REINFORCE\"] = run_episode(MT_Env(data_file=test_data_file, wear_threshold=wear_threshold, r1=r1, r2=r2, r3=r3), rf_action_getter)\n",
    "    else:\n",
    "        results[\"REINFORCE\"] = {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan, \"accuracy\": np.nan, \"avg_margin\": np.nan}\n",
    "\n",
    "    # 3) REINFORCE with Attention\n",
    "    try:\n",
    "        policy_am = PolicyNetwork(am_env.observation_space.shape[0], am_env.action_space.n)\n",
    "        ckpt_am = torch.load(reinforce_am_model_file)\n",
    "        policy_am.load_state_dict(ckpt_am[\"policy_state_dict\"])\n",
    "        policy_am.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading REINFORCE-AM model '{reinforce_am_model_file}': {e}\")\n",
    "        policy_am = None\n",
    "\n",
    "    if policy_am is not None:\n",
    "        def am_action_getter(obs):\n",
    "            with torch.no_grad():\n",
    "                t = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                probs = policy_am(t).squeeze(0).cpu().numpy()\n",
    "                return int(np.argmax(probs))\n",
    "        results[\"REINFORCE_AM\"] = run_episode(AM_Env(data_file=test_data_file, wear_threshold=wear_threshold, r1=r1, r2=r2, r3=r3), am_action_getter)\n",
    "    else:\n",
    "        results[\"REINFORCE_AM\"] = {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan, \"accuracy\": np.nan, \"avg_margin\": np.nan}\n",
    "\n",
    "    # Build DataFrame summary\n",
    "    df = pd.DataFrame.from_dict(results, orient=\"index\")[[\"precision\", \"recall\", \"f1\", \"accuracy\", \"avg_margin\"]]\n",
    "    df.index.name = \"model\"\n",
    "    print(\"\\nPerformance comparison (classification metrics) and avg replacement margins:\")\n",
    "    print(df.round(4))\n",
    "    return df\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0994050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance comparison (classification metrics) and avg replacement margins:\n",
      "              precision  recall   f1  accuracy  avg_margin\n",
      "model                                                     \n",
      "PPO                 0.0     0.0  0.0    0.9808         NaN\n",
      "REINFORCE           0.0     0.0  0.0    0.9808         NaN\n",
      "REINFORCE_AM        0.0     0.0  0.0    0.9808         NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_5108\\405101089.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(reinforce_model_file)\n",
      "C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_5108\\405101089.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt_am = torch.load(reinforce_am_model_file)\n"
     ]
    }
   ],
   "source": [
    "# Example (run in a notebook cell)\n",
    "test_df = test_all_models(\n",
    "    test_data_file=TEST_FILE,\n",
    "    wear_threshold=WEAR_THRESHOLD,\n",
    "    ppo_model_file=PPO_MODEL,\n",
    "    reinforce_model_file=REINFORCE_MODEL,\n",
    "    reinforce_am_model_file=REINFORCE_AM_MODEL\n",
    ")\n",
    "# print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2834b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def test_REINFORCE(model_file: str, test_file: str,\n",
    "                   wear_threshold: float = WEAR_THRESHOLD,\n",
    "                   r1: float = R1_CONTINUE, r2: float = R2_REPLACE, r3: float = R3_VIOLATION):\n",
    "    \"\"\"\n",
    "    Test a saved REINFORCE model on a test CSV (time-series).\n",
    "    - model_file: path to saved checkpoint containing 'policy_state_dict'\n",
    "    - test_file: path to CSV with sensor rows (and an ACTION_CODE column for ground-truth)\n",
    "    Returns: dict with precision, recall, f1, accuracy and predicted_actions list.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "    # create test environment from file\n",
    "    env = MT_Env(data_file=test_file, wear_threshold=wear_threshold, r1=r1, r2=r2, r3=r3)\n",
    "\n",
    "    # load checkpoint\n",
    "    try:\n",
    "        ckpt = torch.load(model_file, map_location=\"cpu\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model '{model_file}': {e}\")\n",
    "        return None\n",
    "\n",
    "    # instantiate policy network matching env\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    policy = PolicyNetwork(input_dim, output_dim)\n",
    "    try:\n",
    "        policy.load_state_dict(ckpt[\"policy_state_dict\"])\n",
    "    except Exception:\n",
    "        # fallback: try to load entire ckpt if saved differently\n",
    "        try:\n",
    "            policy.load_state_dict(ckpt)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load policy state dict: {e}\")\n",
    "            return None\n",
    "    policy.eval()\n",
    "\n",
    "    # run through time-series once, collecting predicted actions\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    predicted_actions = []\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            t = torch.FloatTensor(obs).unsqueeze(0) if obs.ndim == 1 else torch.FloatTensor(obs)\n",
    "            probs = policy(t).squeeze(0).cpu().numpy()\n",
    "            action = int(np.argmax(probs))  # deterministic for testing\n",
    "        predicted_actions.append(action)\n",
    "        # print(f' ---- Pred Action {action}')\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # load ground-truth action codes from CSV\n",
    "    df = pd.read_csv(test_file)\n",
    "    # try common column names\n",
    "    for col in [\"ACTION_CODE\", \"action_code\", \"ACTION\", \"action\"]:\n",
    "        if col in df.columns:\n",
    "            gt_col = col\n",
    "            break\n",
    "    else:\n",
    "        print(\"Ground-truth ACTION column not found. Expected one of: ACTION_CODE, action_code, ACTION, action\")\n",
    "        return None\n",
    "\n",
    "    y_true_full = df[gt_col].astype(int).to_numpy()\n",
    "    # align lengths\n",
    "    n = len(predicted_actions)\n",
    "    y_true = y_true_full[:n]\n",
    "\n",
    "    # compute metrics (handle edge cases)\n",
    "    precision = precision_score(y_true, predicted_actions, zero_division=0)\n",
    "    recall = recall_score(y_true, predicted_actions, zero_division=0)\n",
    "    f1 = f1_score(y_true, predicted_actions, zero_division=0)\n",
    "    acc = accuracy_score(y_true, predicted_actions)\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"predicted_actions\": predicted_actions\n",
    "    }\n",
    "\n",
    "    print(f\"REINFORCE test results — precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, acc: {acc:.4f}\")\n",
    "    return metrics\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a8fe5aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE test results — precision: 0.0000, recall: 0.0000, f1: 0.0000, acc: 0.6154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajesh\\AppData\\Local\\Temp\\ipykernel_5108\\479913281.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "X = test_REINFORCE(REINFORCE_MODEL, TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7815dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
