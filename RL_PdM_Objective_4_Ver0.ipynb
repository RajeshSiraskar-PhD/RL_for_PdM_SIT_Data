{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f03fe92",
   "metadata": {},
   "source": [
    "## RL for Predictive Maintenance\n",
    "\n",
    "- Using SIT data\n",
    "- Tested with Case-1, 200 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, Dict, Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e49a4f",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Parameters ---\n",
    "# These can be easily modified for experimentation.\n",
    "\n",
    "# Data and Environment settings\n",
    "IOT_FILE = 'SIT_84'\n",
    "DATA_FILE = f'{IOT_FILE}.csv' # 'data\\Case_1_1K_Data.csv'\n",
    "TEST_FILE = f'SIT_10.csv' # 'data\\Case_1_Test.csv'\n",
    "WEAR_THRESHOLD = 290.0  # Threshold for tool wear before it's considered failed\n",
    "\n",
    "# Reward structure\n",
    "R1_CONTINUE = 2.0      # Reward for continuing with a healthy tool\n",
    "R2_REPLACE = -5.0      # Reward for a timely replacement\n",
    "R3_VIOLATION = -30.0   # Penalty for violating the wear threshold\n",
    "\n",
    "# Training settings\n",
    "EPISODES = 1000          # Total number of episodes to train each agent\n",
    "LEARNING_RATE = 5e-3    # Learning rate for optimizers\n",
    "GAMMA = 0.99            # Discount factor for future rewards\n",
    "\n",
    "# Plotting settings\n",
    "SMOOTH_WINDOW = 10     # Window size for moving average smoothing on plots\n",
    "\n",
    "# Model saving\n",
    "SAVE_MODEL = True\n",
    "MODEL_FILE = f'models/{IOT_FILE}_Model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80578f62",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 4: Plotting the Comparison ---\n",
    "\n",
    "def smooth(data: List[float], window_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies a simple moving average to a list of data.\n",
    "    Handles NaNs by ignoring them in the window calculation.\n",
    "    \"\"\"\n",
    "    return pd.Series(data).rolling(window=window_size, min_periods=1).mean().to_numpy()\n",
    "\n",
    "def plot_metrics(reinforce_metrics: Dict, ppo_metrics: Dict, window: int, mode: str = \"COMBINED\"):\n",
    "    \"\"\"\n",
    "    Generates performance metric plots either combined or separate.\n",
    "    \n",
    "    Args:\n",
    "        reinforce_metrics (Dict): Dictionary containing REINFORCE metrics\n",
    "        ppo_metrics (Dict): Dictionary containing PPO metrics\n",
    "        window (int): Window size for smoothing the curves\n",
    "        mode (str): Either \"COMBINED\" or \"SEPARATE\" for plotting style\n",
    "    \"\"\"\n",
    "    W, H = 16, 6\n",
    "    FONTSIZE_SUPER = 18\n",
    "    FONTSIZE_TITLE = 12\n",
    "    FONTSIZE_LABEL = 10\n",
    "    FONTSIZE_TICK = 9\n",
    "    BACKGROUND_COLOR = '#f8f8f8'  # Lighter grey background\n",
    "    \n",
    "    def setup_subplot(ax, title, xlabel, ylabel):\n",
    "        \"\"\"Helper function to set up common subplot properties\"\"\"\n",
    "        ax.set_title(title, fontsize=FONTSIZE_TITLE)\n",
    "        ax.set_xlabel(xlabel, fontsize=FONTSIZE_LABEL)\n",
    "        ax.set_ylabel(ylabel, fontsize=FONTSIZE_LABEL)\n",
    "        ax.tick_params(labelsize=FONTSIZE_TICK)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_facecolor(BACKGROUND_COLOR)\n",
    "    \n",
    "    # Get data ranges for consistent y-axis limits\n",
    "    def get_data_ranges(metrics_config):\n",
    "        ranges = {}\n",
    "        for config in metrics_config:\n",
    "            key = config['data']\n",
    "            if key == 'margins':\n",
    "                r_data = pd.Series(reinforce_metrics[key]).rolling(window, min_periods=1).mean()\n",
    "                p_data = pd.Series(ppo_metrics[key]).rolling(window, min_periods=1).mean()\n",
    "            else:\n",
    "                r_data = smooth(reinforce_metrics[key], window)\n",
    "                p_data = smooth(ppo_metrics[key], window)\n",
    "            \n",
    "            all_data = np.concatenate([r_data, p_data])\n",
    "            ranges[key] = (np.nanmin(all_data), np.nanmax(all_data))\n",
    "        return ranges\n",
    "        \n",
    "    if mode.upper() == \"COMBINED\":\n",
    "        # Create single figure with both algorithms\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(W, H))\n",
    "        fig.suptitle('REINFORCE vs. PPO Performance for Predictive Maintenance', fontsize=FONTSIZE_SUPER)\n",
    "        \n",
    "        # Plot all metrics with both algorithms\n",
    "        metrics_config = [\n",
    "            {'data': 'rewards', 'title': 'Learning Curve (Smoothed Average Reward)', \n",
    "             'ylabel': 'Average Reward'},\n",
    "            {'data': 'replacements', 'title': 'Replacements per episode (Smoothed)', \n",
    "             'ylabel': 'Replacement Rate'},\n",
    "            {'data': 'violations', 'title': 'Threshold Violations per episode (Smoothed)', \n",
    "             'ylabel': 'Violation Rate'},\n",
    "            {'data': 'margins', 'title': 'Wear Margin Before Replacement (Smoothed)', \n",
    "             'ylabel': 'Wear Margin'}\n",
    "        ]\n",
    "        \n",
    "        for idx, config in enumerate(metrics_config):\n",
    "            ax = axs[idx // 2, idx % 2]\n",
    "            if config['data'] == 'margins':\n",
    "                r_data = pd.Series(reinforce_metrics[config['data']]).rolling(window, min_periods=1).mean()\n",
    "                p_data = pd.Series(ppo_metrics[config['data']]).rolling(window, min_periods=1).mean()\n",
    "            else:\n",
    "                r_data = smooth(reinforce_metrics[config['data']], window)\n",
    "                p_data = smooth(ppo_metrics[config['data']], window)\n",
    "                \n",
    "            ax.plot(r_data, label='REINFORCE', alpha=0.6)\n",
    "            ax.plot(p_data, label='PPO', alpha=0.6)\n",
    "            setup_subplot(ax, config['title'], 'Episode', config['ylabel'])\n",
    "            ax.legend()\n",
    "        \n",
    "    else:  # SEPARATE mode\n",
    "        # First calculate data ranges for consistent scaling\n",
    "        metrics_config = [\n",
    "            {'data': 'rewards', 'color': 'blue', 'title': 'Learning Curve (Smoothed Average Reward)', \n",
    "             'ylabel': 'Average Reward'},\n",
    "            {'data': 'replacements', 'color': 'green', 'title': 'Replacements per episode (Smoothed)', \n",
    "             'ylabel': 'Replacement Rate'},\n",
    "            {'data': 'violations', 'color': 'red', 'title': 'Threshold Violations per episode (Smoothed)', \n",
    "             'ylabel': 'Violation Rate'},\n",
    "            {'data': 'margins', 'color': 'purple', 'title': 'Wear Margin Before Replacement (Smoothed)', \n",
    "             'ylabel': 'Wear Margin'}\n",
    "        ]\n",
    "        data_ranges = get_data_ranges(metrics_config)\n",
    "        \n",
    "        # Create separate figures for each algorithm\n",
    "        for algo, metrics in [(\"REINFORCE\", reinforce_metrics), (\"PPO\", ppo_metrics)]:\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(W, H))\n",
    "            fig.patch.set_facecolor(BACKGROUND_COLOR)  # Set figure background\n",
    "            fig.suptitle(f'{algo} Performance Metrics for Predictive Maintenance', fontsize=FONTSIZE_SUPER)\n",
    "            \n",
    "            for idx, config in enumerate(metrics_config):\n",
    "                ax = axs[idx // 2, idx % 2]\n",
    "                if config['data'] == 'margins':\n",
    "                    data = pd.Series(metrics[config['data']]).rolling(window, min_periods=1).mean()\n",
    "                else:\n",
    "                    data = smooth(metrics[config['data']], window)\n",
    "                    \n",
    "                ax.plot(data, color=config['color'], alpha=0.6)\n",
    "                setup_subplot(ax, config['title'], 'Episode', config['ylabel'])\n",
    "                \n",
    "                # Set consistent y-axis limits\n",
    "                ymin, ymax = data_ranges[config['data']]\n",
    "                padding = (ymax - ymin) * 0.1  # Add 10% padding\n",
    "                # ax.set_ylim(ymin - padding, ymax + padding)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810d40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68ba8015",
   "metadata": {},
   "source": [
    "### Custom RL Environment for milling tool maintenance\n",
    "- OpenAI's Gymnasium open standards format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Custom Gymnasium Environment ---\n",
    "# We define the environment for the milling tool maintenance problem.\n",
    "\n",
    "class MT_Env(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium environment for Milling Tool Predictive Maintenance.\n",
    "\n",
    "    The agent observes the current tool wear and decides whether to continue\n",
    "    using the tool or replace it.\n",
    "\n",
    "    Observation Space: A single continuous value representing the current tool wear.\n",
    "    Action Space: Two discrete actions - 0 (CONTINUE) or 1 (REPLACE_TOOL).\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, data_file: str, wear_threshold: float, r1: float, r2: float, r3: float):\n",
    "        \"\"\"\n",
    "        Initializes the environment.\n",
    "\n",
    "        Args:\n",
    "            data_file (str): Path to the CSV file with sensor data and tool wear.\n",
    "            wear_threshold (float): The maximum allowable tool wear.\n",
    "            r1 (float): Reward for continuing.\n",
    "            r2 (float): Reward for a timely replacement.\n",
    "            r3 (float): Penalty for a threshold violation.\n",
    "        \"\"\"\n",
    "        super(MT_Env, self).__init__()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        if not os.path.exists(data_file):\n",
    "            raise FileNotFoundError(f\"Data file not found at {data_file}. Please ensure the file is in the correct directory.\")\n",
    "        self.df = pd.read_csv(data_file)\n",
    "        self.max_steps = len(self.df) - 1\n",
    "\n",
    "        # Environment parameters\n",
    "        self.wear_threshold = wear_threshold\n",
    "        self.r1_continue = r1\n",
    "        self.r2_replace = r2\n",
    "        self.r3_violation = r3\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(2)  # 0: CONTINUE, 1: REPLACE_TOOL\n",
    "        # The observation is the current tool wear, normalized for better learning\n",
    "        self.observation_space = spaces.Box(low=0, high=self.wear_threshold * 1.5, shape=(1,), dtype=np.float32)\n",
    "\n",
    "        # Internal state\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state for a new episode.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        initial_observation = self._get_observation()\n",
    "        return initial_observation, {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment.\n",
    "        \"\"\"\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "        info = {'violation': False, 'replacement': False, 'margin': np.nan}\n",
    "\n",
    "        current_wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "\n",
    "        if action == 1:  # Action: REPLACE_TOOL\n",
    "            reward = self.r2_replace\n",
    "            terminated = True\n",
    "            info['replacement'] = True\n",
    "            info['margin'] = self.wear_threshold - current_wear\n",
    "        elif action == 0:  # Action: CONTINUE\n",
    "            self.current_step += 1\n",
    "            if self.current_step > self.max_steps:\n",
    "                # Reached the end of data without replacement or violation\n",
    "                truncated = True\n",
    "                reward = 0 # No reward or penalty if data runs out\n",
    "            else:\n",
    "                next_wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "                if next_wear >= self.wear_threshold:\n",
    "                    reward = self.r3_violation\n",
    "                    terminated = True\n",
    "                    info['violation'] = True\n",
    "                    # $$$ Avoid NANs for margin. Even if NOT replaced, see what the margin is\n",
    "                    info['margin'] = self.wear_threshold - current_wear\n",
    "                else:\n",
    "                    reward = self.r1_continue\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gets the current observation from the dataframe.\n",
    "        \"\"\"\n",
    "        wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "        return np.array([wear], dtype=np.float32)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Renders the environment state (optional, for visualization).\n",
    "        \"\"\"\n",
    "        wear = self.df.loc[self.current_step, 'tool_wear']\n",
    "        print(f\"Step: {self.current_step}, Tool Wear: {wear:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38c15b",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm Implementation\n",
    "- Custom implementation of the REINFORCE algorithm\n",
    "- Employs OpenAI's Stable Baselines3 open-source standards for wider research adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 2: REINFORCE Algorithm Implementation ---\n",
    "# A custom implementation of the REINFORCE algorithm with a structure\n",
    "# similar to Stable Baselines3 for easy comparison.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.net(x)\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    A custom REINFORCE agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, policy, env, learning_rate=0.001, gamma=0.99, model_file=None):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.model_name = \"REINFORCE\"\n",
    "        self.model_file = model_file\n",
    "\n",
    "    def predict(self, obs: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current observation.\n",
    "        Uses the same action selection logic as training for consistency.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Important to disable gradients for prediction\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            probs = self.policy(state_tensor)\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()  # Sample action like in training\n",
    "            return action.item()        \n",
    "\n",
    "    def learn(self, total_episodes: int) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        Trains the agent for a given number of episodes.\n",
    "        \"\"\"\n",
    "        print(f\"--- Training {self.model_name} ---\")\n",
    "        # Metrics collectors\n",
    "        all_rewards = []\n",
    "        all_violations = []\n",
    "        all_replacements = []\n",
    "        all_margins = []\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            episode_info = {'violation': 0, 'replacement': 0, 'margin': np.nan}\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "                probs = self.policy(state_tensor)\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample()\n",
    "\n",
    "                log_prob = dist.log_prob(action)\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "                obs, reward, terminated, truncated, info = self.env.step(action.item())\n",
    "                rewards.append(reward)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if info.get('violation'): episode_info['violation'] = 1\n",
    "                if info.get('replacement'): episode_info['replacement'] = 1\n",
    "                if not np.isnan(info.get('margin')): episode_info['margin'] = info.get('margin')\n",
    "            \n",
    "            # Collect metrics for this episode\n",
    "            all_rewards.append(sum(rewards))\n",
    "            all_violations.append(episode_info['violation'])\n",
    "            all_replacements.append(episode_info['replacement'])\n",
    "            all_margins.append(episode_info['margin'])\n",
    "\n",
    "            # Calculate discounted returns\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32)\n",
    "            \n",
    "            # Normalize returns for stability, but only if there's more than one step\n",
    "            # to avoid std() of a single item becoming NaN.\n",
    "            if len(returns) > 1:\n",
    "                returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "            # Calculate policy loss and update weights\n",
    "            policy_loss = []\n",
    "            for log_prob, G in zip(log_probs, returns):\n",
    "                policy_loss.append(-log_prob * G)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss = torch.cat(policy_loss).sum()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}/{total_episodes}, Total Reward: {sum(rewards):.2f}\")\n",
    "        \n",
    "        print(\"--- Training Complete ---\")\n",
    "        \n",
    "        # Save model if file path was provided\n",
    "        if self.model_file is not None:\n",
    "            try:\n",
    "                # Create directory if it doesn't exist\n",
    "                os.makedirs(os.path.dirname(self.model_file), exist_ok=True)\n",
    "                # Save the model\n",
    "                torch.save({\n",
    "                    'policy_state_dict': self.policy.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'input_dim': self.policy.net[0].in_features,\n",
    "                    'output_dim': self.policy.net[-2].out_features\n",
    "                }, self.model_file)\n",
    "                print(f\"Model saved successfully to: {self.model_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            \"rewards\": all_rewards,\n",
    "            \"violations\": all_violations,\n",
    "            \"replacements\": all_replacements,\n",
    "            \"margins\": all_margins\n",
    "        }\n",
    "    \n",
    "    \n",
    "\n",
    "# --- Task 3: Comparison with Stable Baselines3 PPO ---\n",
    "# We use a callback to collect metrics during PPO training to ensure a\n",
    "# fair, episode-by-episode comparison with our REINFORCE agent.\n",
    "\n",
    "class MetricsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback for saving metrics at the end of each episode.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(MetricsCallback, self).__init__(verbose)\n",
    "        self.rewards = []\n",
    "        self.violations = []\n",
    "        self.replacements = []\n",
    "        self.margins = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if an episode has ended\n",
    "        if self.locals['dones'][0]:\n",
    "            # The Monitor wrapper ensures the info dict has 'episode' stats\n",
    "            info = self.locals['infos'][0]\n",
    "            self.rewards.append(info['episode']['r'])\n",
    "            self.violations.append(1 if info.get('violation') else 0)\n",
    "            self.replacements.append(1 if info.get('replacement') else 0)\n",
    "            self.margins.append(info.get('margin', np.nan))\n",
    "        return True\n",
    "\n",
    "def train_ppo(env: gym.Env, total_episodes: int) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Trains a PPO agent and collects metrics using a callback.\n",
    "    \"\"\"\n",
    "    print(\"--- Training PPO ---\")\n",
    "    callback = MetricsCallback()\n",
    "    \n",
    "    # Since we train by episode, we set a very high number of timesteps\n",
    "    # and rely on the custom loop to stop at the right number of episodes.\n",
    "    total_timesteps = 1_000_000 \n",
    "\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0, learning_rate=LEARNING_RATE, gamma=GAMMA)\n",
    "    \n",
    "    # We can now use the standard model.learn() with our callback,\n",
    "    # but we need a way to stop after N episodes. A custom loop is still clearer.\n",
    "    obs, _ = env.reset()\n",
    "    ep_count = 0\n",
    "    while ep_count < total_episodes:\n",
    "        action, _states = model.predict(obs, deterministic=False)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Manually trigger the callback's on_step method\n",
    "        callback.model = model\n",
    "        callback.locals = {'dones': [terminated or truncated], 'infos': [info]}\n",
    "        callback._on_step()\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            ep_count += 1\n",
    "            if (ep_count) % 100 == 0:\n",
    "                print(f\"Episode {ep_count}/{total_episodes}\")\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    print(\"--- Training Complete ---\")\n",
    "    return {\n",
    "        \"rewards\": callback.rewards,\n",
    "        \"violations\": callback.violations,\n",
    "        \"replacements\": callback.replacements,\n",
    "        \"margins\": callback.margins\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03008b",
   "metadata": {},
   "source": [
    "### Test saved RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_saved_model(model_file: str, test_data_file: str, wear_threshold: float, r1: float, r2: float, r3: float):\n",
    "    \"\"\"\n",
    "    Tests a saved model on new sensor data and visualizes the results.\n",
    "    \n",
    "    Args:\n",
    "        model_file (str): Path to the saved model file\n",
    "        test_data_file (str): Path to the new sensor data file\n",
    "        wear_threshold (float): Threshold for tool wear\n",
    "        r1, r2, r3 (float): Reward parameters for the environment\n",
    "    \"\"\"\n",
    "    # Create test environment\n",
    "    test_env = MT_Env(\n",
    "        data_file=test_data_file,\n",
    "        wear_threshold=wear_threshold,\n",
    "        r1=r1, r2=r2, r3=r3\n",
    "    )\n",
    "    \n",
    "    # Load the model\n",
    "    policy = PolicyNetwork(\n",
    "        input_dim=test_env.observation_space.shape[0],\n",
    "        output_dim=test_env.action_space.n\n",
    "    )\n",
    "    agent = REINFORCE(policy, test_env)\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(model_file)\n",
    "        agent.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        print(f\"Model loaded successfully from: {model_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Test metrics\n",
    "    total_steps = 0\n",
    "    total_replacements = 0\n",
    "    total_violations = 0\n",
    "    margins = []\n",
    "    replacement_points = []\n",
    "    violation_points = []\n",
    "    wear_trajectory = []\n",
    "    \n",
    "    # Run test episode\n",
    "    obs, _ = test_env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "        wear = obs[0]\n",
    "        wear_trajectory.append(wear)\n",
    "        \n",
    "        if info.get('replacement'):\n",
    "            total_replacements += 1\n",
    "            replacement_points.append((step, wear))\n",
    "            margins.append(wear_threshold - wear)\n",
    "        \n",
    "        if info.get('violation'):\n",
    "            total_violations += 1\n",
    "            violation_points.append((step, wear))\n",
    "            margins.append(wear_threshold - wear)\n",
    "        \n",
    "        step += 1\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    total_steps = step\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'total_steps': total_steps,\n",
    "        'replacement_rate': total_replacements / total_steps,\n",
    "        'violation_rate': total_violations / total_steps,\n",
    "        'avg_margin': np.mean(margins) if margins else np.nan,\n",
    "        'min_margin': np.min(margins) if margins else np.nan\n",
    "    }\n",
    "    \n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    fig.suptitle('Model Test Results', fontsize=11)\n",
    "    \n",
    "    # Plot 1: Wear trajectory with events\n",
    "    ax1.plot(wear_trajectory, 'b-', alpha=0.6, label='Tool Wear')\n",
    "    ax1.axhline(y=wear_threshold, color='r', linestyle='--', alpha=0.6, label='Wear Threshold')\n",
    "    \n",
    "    # Plot replacement and violation points\n",
    "    if replacement_points:\n",
    "        points = np.array(replacement_points)\n",
    "        ax1.scatter(points[:, 0], points[:, 1], color='g', marker='^', \n",
    "                label='Replacements', alpha=0.6)\n",
    "    if violation_points:\n",
    "        points = np.array(violation_points)\n",
    "        ax1.scatter(points[:, 0], points[:, 1], color='r', marker='x', \n",
    "                label='Violations', alpha=0.6)\n",
    "    \n",
    "    ax1.set_title('Tool Wear Trajectory')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Wear')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Margins histogram\n",
    "    if margins:\n",
    "        ax2.hist(margins, bins=20, alpha=0.6, color='purple')\n",
    "        ax2.axvline(x=0, color='r', linestyle='--', alpha=0.6)\n",
    "        ax2.set_title('Distribution of Margins at Decision Points')\n",
    "        ax2.set_xlabel('Margin (Threshold - Wear)')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Total Steps: {metrics['total_steps']}\")\n",
    "    print(f\"Replacement Rate: {metrics['replacement_rate']:.4f}\")\n",
    "    print(f\"Violation Rate: {metrics['violation_rate']:.4f}\")\n",
    "    print(f\"Average Margin: {metrics['avg_margin']:.2f}\")\n",
    "    print(f\"Minimum Margin: {metrics['min_margin']:.2f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a39de",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf25b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "try:\n",
    "    # Create a base environment for the custom REINFORCE agent\n",
    "    reinforce_env = MT_Env(\n",
    "        data_file=DATA_FILE,\n",
    "        wear_threshold=WEAR_THRESHOLD,\n",
    "        r1=R1_CONTINUE,\n",
    "        r2=R2_REPLACE,\n",
    "        r3=R3_VIOLATION\n",
    "    )\n",
    "    # It's good practice to check if the custom environment is valid\n",
    "    check_env(reinforce_env)\n",
    "    print(\"Environment created and checked successfully.\")\n",
    "\n",
    "    # Create a separate, monitored environment for the Stable Baselines3 PPO agent\n",
    "    ppo_env = Monitor(MT_Env(\n",
    "        data_file=DATA_FILE,\n",
    "        wear_threshold=WEAR_THRESHOLD,\n",
    "        r1=R1_CONTINUE,\n",
    "        r2=R2_REPLACE,\n",
    "        r3=R3_VIOLATION\n",
    "    ))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    # Exit if the data file is not found\n",
    "    exit()\n",
    "\n",
    "# Train REINFORCE agent\n",
    "print('\\n\\n Training REINFORCE agent...')\n",
    "reinforce_policy = PolicyNetwork(reinforce_env.observation_space.shape[0], reinforce_env.action_space.n)\n",
    "reinforce_agent = REINFORCE(reinforce_policy, reinforce_env, learning_rate=LEARNING_RATE, gamma=GAMMA, model_file=MODEL_FILE)\n",
    "reinforce_metrics = reinforce_agent.learn(total_episodes=EPISODES)\n",
    "print('\\n -- REINFORCE agent training complete.')\n",
    "\n",
    "# Train PPO agent\n",
    "print('\\n\\n Training PPO agent...')\n",
    "ppo_metrics = train_ppo(ppo_env, total_episodes=EPISODES)\n",
    "print('\\n -- PPO agent training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025a29a",
   "metadata": {},
   "source": [
    "### Results plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf15fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTH_WINDOW = 2\n",
    "# For combined plots (both algorithms on same axes)\n",
    "plot_metrics(reinforce_metrics, ppo_metrics, window=SMOOTH_WINDOW, mode=\"COMBINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89052bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For separate plots (each algorithm on its own figure)\n",
    "# plot_metrics(reinforce_metrics, ppo_metrics, window=SMOOTH_WINDOW, mode=\"SEPARATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1122f8",
   "metadata": {},
   "source": [
    "### Test model saving and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = test_saved_model(\n",
    "    model_file=MODEL_FILE,\n",
    "    test_data_file=TEST_FILE,\n",
    "    wear_threshold=WEAR_THRESHOLD,\n",
    "    r1=R1_CONTINUE,\n",
    "    r2=R2_REPLACE,\n",
    "    r3=R3_VIOLATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7138c",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe885c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from utilities import downsample\n",
    "# # Read the CSV file\n",
    "\n",
    "# DATA = 'Case_1_1K_Data'\n",
    "# DATA_TO_DOWNSAMPLE = f'data\\{DATA}.csv'\n",
    "# DATA_TO_DOWNSAMPLE_DS = f'data\\DS_{DATA}.csv'\n",
    "\n",
    "# df_raw = pd.read_csv(DATA_TO_DOWNSAMPLE)\n",
    "# df = downsample(df_raw, 20)\n",
    "# df.to_csv(DATA_TO_DOWNSAMPLE_DS, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
